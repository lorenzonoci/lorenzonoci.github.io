---
layout: archive
title: "Publications"
permalink: /publications/
author_profile: true
---

1. "Understanding and Minimising Outlier Features in Neural Network Training."
    Bobby He, **Lorenzo Noci**, Daniele Paliotta, Imanol Schlag, Thomas Hofmann. *Advances in Neural Information Processing Systems (NeurIPS)*, 2024. [arXiv](https://arxiv.org/abs/2405.19279)

2. "Super Consistency of Neural Network Landscapes and Learning Rate Transfer." **Lorenzo Noci**†, Alex Meterez†, Thomas Hofmann, Antonio Orvieto. *Advances in Neural Information Processing Systems (NeurIPS)*, 2024. [arXiv](https://arxiv.org/abs/2402.17457)

3. "Exploring the Limits of Feature Learning in Continual Learning." Jacopo Graldi†, Giulia Lanzillotta, **Lorenzo Noci**, Benjamin Grewe, Thomas Hofmann. *Continual FoMo Workshop at NeurIPS*, 2024. [OpenReview](https://openreview.net/forum?id=TYPBYgWyw8)

4. "Depthwise Hyperparameter Transfer in Residual Networks: Dynamics and Scaling Limit." Blake Bordelon†, **Lorenzo Noci**†, Mufan Bill Li, Boris Hanin, Cengiz Pehlevan. *International Conference on Learning Representations (ICLR)*, 2024. [arXiv](https://arxiv.org/abs/2309.16620)

5. "How Good is a Single Basin?" Kai Lion, **Lorenzo Noci**, Thomas Hofmann, Gregor Bachmann. *Artificial Intelligence and Statistics (AISTATS)*, 2024. [arXiv](https://arxiv.org/abs/2402.03187)

6. "Dynamic Context Pruning for Efficient and Interpretable Autoregressive Transformers." (Spotlight) Sotiris Anagnostidis, Dario Pavllo, Luca Biggio, **Lorenzo Noci**, Aurelien Lucchi, Thomas Hofmann. *Advances in Neural Information Processing Systems (NeurIPS)*, 2023. [arXiv](https://arxiv.org/abs/2310.07887)

7. "The Shaped Transformer: Attention Models in the Infinite Depth-and-Width Limit." **Lorenzo Noci**†, Chuning Li†, Mufan Bill Li†, Bobby He, Thomas Hofmann, Chris Maddison, Daniel M. Roy. *Advances in Neural Information Processing Systems (NeurIPS)*, 2023. [arXiv](https://arxiv.org/abs/2307.06635)

8. "Disentangling Linear Mode-Connectivity." Gül Sena Altıntaş, Gregor Bachmann, **Lorenzo Noci**, Thomas Hofmann. *UniReps Workshop at NeurIPS*, 2023. [arXiv](https://arxiv.org/abs/2308.10634)

9. "Achieving a Better Stability-Plasticity Trade-off via Auxiliary Networks in Continual Learning." Sanghwan Kim†, **Lorenzo Noci**, Antonio Orvieto, and Thomas Hofmann. *IEEE/CVF Conference on Computer Vision and Pattern Recognition*, 2023. [arXiv](https://arxiv.org/abs/2305.10452)

10. "The Curious Case of Benign Memorization." Sotiris Anagnostidis†, Gregor Bachmann†, **Lorenzo Noci**†, and Thomas Hofmann. *International Conference on Learning Representations (ICLR)*, 2023. [arXiv](https://arxiv.org/abs/2303.03627)

11. "Signal Propagation in Transformers: Theoretical Perspectives and the Role of Rank Collapse." **Lorenzo Noci**†, Sotiris Anagnostidis†, Luca Biggio†, Antonio Orvieto†, Sidak Pal Singh†, and Aurelien Lucchi. *Advances in Neural Information Processing Systems (NeurIPS)*, 2022. [arXiv](https://arxiv.org/abs/2205.04024)

12. "How Tempering Fixes Data Augmentation in Bayesian Neural Networks." (Oral) Gregor Bachmann†, **Lorenzo Noci**†, and Thomas Hofmann. *International Conference on Machine Learning*, pages 1244–1260, PMLR, 2022. [arXiv](https://arxiv.org/abs/2202.07342)

13. "Precise Characterization of the Prior Predictive Distribution of Deep ReLU Networks." (Spotlight) **Lorenzo Noci**†, Gregor Bachmann†, Kevin Roth†, Sebastian Nowozin, and Thomas Hofmann. *Advances in Neural Information Processing Systems*, 34, 2021. [arXiv](https://arxiv.org/abs/2112.00898)

14. "Disentangling the Roles of Curation, Data-Augmentation and the Prior in the Cold Posterior Effect." **Lorenzo Noci**†, Kevin Roth†, Gregor Bachmann†, Sebastian Nowozin, and Thomas Hofmann. *Advances in Neural Information Processing Systems (NeurIPS)*, 34, 2021. [arXiv](https://arxiv.org/abs/2112.08448)

15. "Adversarial Learning for Debiasing Knowledge Graph Embeddings." Mario Arduini†, **Lorenzo Noci**†, Federico Pirovano†, Ce Zhang, Yash Raj Shrestha, and Bibek Paudel. *Workshop on Mining and Learning with Graphs at KDD.* [arXiv](https://arxiv.org/abs/2105.07856)

